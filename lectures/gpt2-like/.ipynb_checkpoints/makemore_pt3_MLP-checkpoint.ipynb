{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "51d2b163-3946-4f24-8d7c-5003ab66311c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "37ec6ef8-c56c-433e-aba7-5f05269fc3f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma', 'olivia']"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = open('names.txt', 'r').read().splitlines()\n",
    "words[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "831593db-20d4-4956-a2d3-d3d3b2ee79f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(''.join(words))))\n",
    "chars.insert(0, '.')\n",
    "\n",
    "stoi = {c:i for i,c in enumerate(chars)}\n",
    "itos = { i:c for i,c in enumerate(chars)}\n",
    "vocab_size = len(itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "861d05f2-0326-4862-aafe-e2c7e25b754e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([156999, 3]) torch.Size([156999])\n",
      "torch.Size([19452, 3]) torch.Size([19452])\n",
      "torch.Size([19662, 3]) torch.Size([19662])\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "block_size = 3\n",
    "\n",
    "def build_dataset(words):\n",
    "    X, Y = [], []\n",
    "    \n",
    "    for w in words:\n",
    "        # print(w)\n",
    "        context = [0] * block_size\n",
    "        for ch in w:\n",
    "            ix = stoi[ch]\n",
    "            X.append(context)\n",
    "    \n",
    "            Y.append(ix)\n",
    "            # print(''.join(itos[i] for i in context), ' --->', itos[ix])\n",
    "            context = context[1:] + [ix]\n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    print(X.shape, Y. shape)\n",
    "    return X, Y\n",
    "\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "\n",
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "\n",
    "\n",
    "Xtr, Ytr = build_dataset(words[:n1])\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])\n",
    "Xte, Yte = build_dataset(words[n2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "3e245eb4-0fec-4935-9cf3-f3d0532f810d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of params: 12097\n"
     ]
    }
   ],
   "source": [
    "n_embed = 10\n",
    "n_hidden = 200\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "C = torch.randn((vocab_size, n_embed), generator=g) # 2 col embedding vector for each character\n",
    "W1 = torch.randn((block_size * n_embed, n_hidden), generator=g) * (5/3)/((block_size * n_embed)**0.5) # Normalize/reduce values of W1 to avoid saturation of tanh activation\n",
    "# but in there embedding form (so 3 embeddings flattened). 100 out is design choice\n",
    "# b1 = torch.randn(n_hidden, generator=g) * 0.01 ## setting biases to close to 0, as at step 0, we want to avoid having probabilities > 1/vocab_size\n",
    " ## because why should an untrained network know to output one character over another\n",
    "W2 = torch.randn((n_hidden, vocab_size), generator=g) * 0.01 ## we reduce weights by factor of 10^-2 for similar reasons here\n",
    "## Because if resultant logits are large, it implies false confidence in specific characters. Unless the Y indexes pick out those high values\n",
    "## the resultant loss will be large\n",
    "b2 = torch.randn(vocab_size, generator=g) * 0 \n",
    "\n",
    "bngain = torch.ones((1, n_hidden))\n",
    "bnbias = torch.zeros((1, n_hidden))\n",
    "\n",
    "bnmean_running = torch.ones((1, n_hidden))\n",
    "bnstd_running = torch.ones((1, n_hidden))\n",
    "\n",
    "parameters = [C, W1, W2, b2, bngain, bnbias]\n",
    "print(f\"Total number of params: {sum(p.nelement() for p in parameters)}\")\n",
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "5531d2a7-4edd-4c3c-8a47-649e2f200090",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # if we are lucky with inital random assignment\n",
    "# logits1 = torch.tensor([0.0, 2.0, -0.3, -10.1])\n",
    "# probs1 = torch.softmax(logits1, dim=0)\n",
    "# loss1 = -probs1[1].log() #1 is the training Y index, and is fortunately large, so loss will be low\n",
    "\n",
    "\n",
    "# #bad logits due to large inital weights example -> leads to large loss we have to mediate in first N steps (uncessary work!)\n",
    "# logits2 = torch.tensor([0.0, 7.0, -3.0, 1.0])\n",
    "# exp2 = logits2.exp()\n",
    "# probs2 =  torch.softmax(logits2, dim=0)\n",
    "# loss2 = probs2[2].log() #here we assume the correct output char is at index 2\n",
    "# logits1, probs1, loss1, logits2, exp2, probs2, loss2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "d0a31cf9-3ca7-4f22-8157-796191743c85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0/ 200000: 3.3023\n",
      "  10000/ 200000: 2.3210\n",
      "  20000/ 200000: 2.0799\n",
      "  30000/ 200000: 2.4238\n",
      "  40000/ 200000: 2.2365\n",
      "  50000/ 200000: 2.1992\n",
      "  60000/ 200000: 2.3269\n",
      "  70000/ 200000: 2.4307\n",
      "  80000/ 200000: 2.3572\n",
      "  90000/ 200000: 1.9672\n",
      " 100000/ 200000: 2.2255\n",
      " 110000/ 200000: 2.0786\n",
      " 120000/ 200000: 2.3213\n",
      " 130000/ 200000: 2.4597\n",
      " 140000/ 200000: 2.2014\n",
      " 150000/ 200000: 2.2182\n",
      " 160000/ 200000: 2.5412\n",
      " 170000/ 200000: 1.8623\n",
      " 180000/ 200000: 1.8665\n",
      " 190000/ 200000: 1.6702\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# lre = torch.linspace(-3, 0, epochs)\n",
    "# lrs = 10**lre\n",
    "# lri = []\n",
    "\n",
    "batch_size = 32\n",
    "max_steps = 200000\n",
    "\n",
    "lossi = []\n",
    "# iz = []\n",
    "\n",
    "\n",
    "for i in range(max_steps):\n",
    "    ## construct minibatch indexes\n",
    "    ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "    Xb, Yb = Xtr[ix], Ytr[ix]\n",
    "    ## forward pass\n",
    "    emb = C[Xb]\n",
    "    embcat = emb.view(emb.shape[0], -1)\n",
    "    hpreact = embcat @ W1 #+ b1 #when using batch normalization, bnmean effectively strips the layer bias out and bnbias takes over this role, so we can remove it\n",
    "        ## BatchNorm Layer\n",
    "    bnmeani = hpreact.mean(0, keepdim=True)\n",
    "    bnstdi = hpreact.std(0, keepdim=True)\n",
    "    \n",
    "    hpreact =  bngain * ((hpreact - bnmeani)/ bnstdi) + bnbias\n",
    "\n",
    "        #accumulate estimate for bnmean and bnstd for entire training set\n",
    "    with torch.no_grad():\n",
    "        bnmean_running = 0.999 * bnmean_running + 0.001 * bnmeani\n",
    "        bnstd_running = 0.999 * bnstd_running + 0.001 * bnstdi\n",
    "        \n",
    "    h = torch.tanh( hpreact) # (word-grams, 100)\n",
    "    logits = h @ W2 + b2 # (100, 27)\n",
    "    loss = F.cross_entropy(logits, Yb)\n",
    "    # print(f\"Loss at epoch {k}: {(loss.item()):.4f}, lr was {lrs[k]:.4f}\") \n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    lr = 0.1 if i < 100000 else 0.01\n",
    "    # lr = 0.0001\n",
    "    #update\n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad\n",
    "\n",
    "    # #track stats\n",
    "    if i % 10000 == 0:\n",
    "        print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
    "    lossi.append(loss.log10().item())\n",
    "    # lri.append(lrs[k])\n",
    "    # break  \n",
    "# plt.plot(lri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "bfb879df-e8a5-4090-8c27-fa7fb0f19c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import math\n",
    "# import numpy as np\n",
    "# xx = np.arange(-12, 12, 0.1)\n",
    "# yy = np.tanh(xx)\n",
    "\n",
    "# plt.plot(xx, yy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "0bda5246-2ad8-47b8-9946-2de5d4767d2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Cell magic `%%skip` not found.\n"
     ]
    }
   ],
   "source": [
    "%%skip\n",
    "## Without normalization, majority of values in h are -1 and 1 (tanh 'squashes' between -1 and 1)\n",
    "## recalling that gradient of tanh => (1 - x**2) * grad, if any value of h is 1 or -1, the resultant gradient will be 0 (I.e. the gradient will be dead from then on!)\n",
    "## this makes sense, as, for large values above tanh squashes values to 1 or -1 and stops. So gradient in that region IS 0.\n",
    "\n",
    "# ##for this reason, we need to normalize\n",
    "plt.hist(h.view(-1).tolist(), 50);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "70e7f5ea-3747-46ea-a3c2-17ea47e57928",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# WITH normalization of weights:\n",
    "# ##for this reason, we need to normalize\n",
    "plt.hist(h.view(-1).tolist(), 50);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "d53da973-0c58-4978-ab64-2e596b75dea5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# (with W1 randomly initialized, no normalization) if we inspect the activations flowing into tanh, we can see that many values sit above the tanh thresholds, and will result in many outputs of 1 or -1!\n",
    "# otherwise, we see a histogram that sits nicely without bounds of -1 and 1\n",
    "plt.hist(hpreact.view(-1).tolist(), 50);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "94d1a1ce-8d2c-4326-a7ba-9d4a64bdcdc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x16f4cce0f80>"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABj0AAAElCAYAAAC/JSDoAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAg+klEQVR4nO3df4zUdX4/8NcoMgfe7lz3cH9MWLZbgk0rnDnRIvTkh5Gt2x4n6l21XCzkLPF6QLPhiFSNOa69sKfmiEk5jdd4VKsW00TUVKu3Rlk0lAZBex53odhbhYu7UgnuAMcNCJ/vH/dl6ri47K677PrZxyP5JHzen/dn5jXJvuc9y3Pf884kSZIEAAAAAADAp9w5w10AAAAAAADAYBB6AAAAAAAAqSD0AAAAAAAAUkHoAQAAAAAApILQAwAAAAAASAWhBwAAAAAAkApCDwAAAAAAIBWEHgAAAAAAQCoIPQAAAAAAgFQYM9wFfNTJkyfjnXfeiYqKishkMsNdDgAAAAAAMIySJIlDhw5FPp+Pc87pfS3HkIUe9913X9xzzz3R2dkZF110Udx7771xxRVXnPG+d955J+rr64eqLAAAAAAA4FNo3759MXHixF77DMnXWz3++OPR0tISd9xxR7z22mtxxRVXRHNzc+zdu/eM91ZUVAxFSQAAAAAAwKdYX/KDTJIkyWA/8YwZM+KSSy6J+++/v9T2B3/wB7Fw4cJobW3t9d5CoRC5XG6wSwIAAAAAAD7Furu7o7Kystc+g77S49ixY7Fjx45oamoqa29qaoqtW7f26F8sFqNQKJQdAAAAAAAA/TXoocd7770XJ06ciJqamrL2mpqa6Orq6tG/tbU1crlc6bCfBwAAAAAAMBBDsqdHREQmkyk7T5KkR1tExG233Rbd3d2lY9++fUNVEgAAAAAAkGJjBvsBJ0yYEOeee26PVR379+/vsfojIiKbzUY2mx3sMgAAAAAAgFFm0Fd6jB07NqZPnx5tbW1l7W1tbTFr1qzBfjoAAAAAAICIGIKVHhERK1eujJtuuikuvfTSmDlzZvzoRz+KvXv3xje/+c2heDoAAAAAAIChCT1uuOGGOHDgQPzd3/1ddHZ2xtSpU+PZZ5+NhoaGoXg6AAAAAACAyCRJkgx3ER9WKBQil8sNdxkAAAAAAMAI0t3dHZWVlb32GfQ9PQAAAAAAAIaD0AMAAAAAAEgFoQcAAAAAAJAKQg8AAAAAACAVhB4AAAAAAEAqCD0AAAAAAIBUEHoAAAAAAACpIPQAAAAAAABSQegBAAAAAACkgtADAAAAAABIBaEHAAAAAACQCkIPAAAAAAAgFYQeAAAAAABAKgg9AAAAAACAVBB6AAAAAAAAqSD0AAAAAAAAUkHoAQAAAAAApILQAwAAAAAASAWhBwAAAAAAkApCDwAAAAAAIBWEHgAAAAAAQCoIPQAAAAAAgFQQegAAAAAAAKkg9AAAAAAAAFJB6AEAAAAAAKSC0AMAAAAAAEgFoQcAAAAAAJAKQg8AAAAAACAVBj30WLNmTWQymbKjtrZ2sJ8GAAAAAACgzJiheNCLLrooXnjhhdL5ueeeOxRPAwAAAAAAUDIkoceYMWOs7gAAAAAAAM6qIdnTY8+ePZHP56OxsTFuvPHG+OUvf/mxfYvFYhQKhbIDAAAAAACgvwY99JgxY0Y8/PDD8fzzz8c//uM/RldXV8yaNSsOHDhw2v6tra2Ry+VKR319/WCXBAAAAAAAjAKZJEmSoXyCI0eOxOTJk+PWW2+NlStX9rheLBajWCyWzguFguADAAAAAAAo093dHZWVlb32GZI9PT7s/PPPj2nTpsWePXtOez2bzUY2mx3qMgAAAAAAgJQbkj09PqxYLMYvfvGLqKurG+qnAgAAAAAARrFBDz1WrVoV7e3t0dHREf/5n/8ZX/3qV6NQKMTixYsH+6kAAAAAAABKBv3rrX71q1/FX/zFX8R7770XF1xwQVx++eWxbdu2aGhoGOynAgAAAAAAKBnyjcz7q1AoRC6XG+4yAAAAAACAEaQvG5kP+Z4eAAAAAAAAZ4PQAwAAAAAASAWhBwAAAAAAkApCDwAAAAAAIBWEHgAAAAAAQCoIPQAAAAAAgFQQegAAAAAAAKkg9AAAAAAAAFJB6AEAAAAAAKSC0AMAAAAAAEgFoQcAAAAAAJAKQg8AAAAAACAVhB4AAAAAAEAqCD0AAAAAAIBUEHoAAAAAAACpIPQAAAAAAABSQegBAAAAAACkgtADAAAAAABIBaEHAAAAAACQCkIPAAAAAAAgFYQeAAAAAABAKgg9AAAAAACAVBB6AAAAAAAAqSD0AAAAAAAAUkHoAQAAAAAApILQAwAAAAAASAWhBwAAAAAAkAr9Dj22bNkSCxYsiHw+H5lMJp588smy60mSxJo1ayKfz8e4ceNi7ty5sWvXrsGqFwAAAAAA4LT6HXocOXIkLr744li/fv1pr999992xbt26WL9+fWzfvj1qa2tj/vz5cejQoU9cLAAAAAAAwMfJJEmSDPjmTCY2bdoUCxcujIjfrvLI5/PR0tISq1evjoiIYrEYNTU1cdddd8Utt9xyxscsFAqRy+UGWhIAAAAAAJBC3d3dUVlZ2WufQd3To6OjI7q6uqKpqanUls1mY86cObF169bBfCoAAAAAAIAyYwbzwbq6uiIioqampqy9pqYm3n777dPeUywWo1gsls4LhcJglgQAAAAAAIwSg7rS45RMJlN2niRJj7ZTWltbI5fLlY76+vqhKAkAAAAAAEi5QQ09amtrI+L/Vnycsn///h6rP0657bbboru7u3Ts27dvMEsCAAAAAABGiUENPRobG6O2tjba2tpKbceOHYv29vaYNWvWae/JZrNRWVlZdgAAAAAAAPRXv/f0OHz4cLz55pul846Ojnj99dejqqoqJk2aFC0tLbF27dqYMmVKTJkyJdauXRvjx4+PRYsWDWrhAAAAAAAAH9bv0OPVV1+NefPmlc5XrlwZERGLFy+Of/qnf4pbb701jh49Gt/61rfi4MGDMWPGjPjJT34SFRUVg1c1AAAAAADAR2SSJEmGu4gPKxQKkcvlhrsMAAAAAABgBOnu7j7jFhmDuqcHAAAAAADAcBF6AAAAAAAAqSD0AAAAAAAAUkHoAQAAAAAApILQAwAAAAAASAWhBwAAAAAAkApCDwAAAAAAIBWEHgAAAAAAQCoIPQAAAAAAgFQQegAAAAAAAKkg9AAAAAAAAFJB6AEAAAAAAKSC0AMAAAAAAEgFoQcAAAAAAJAKQg8AAAAAACAVhB4AAAAAAEAqCD0AAAAAAIBUEHoAAAAAAACpIPQAAAAAAABSQegBAAAAAACkgtADAAAAAABIBaEHAAAAAACQCkIPAAAAAAAgFYQeAAAAAABAKgg9AAAAAACAVBB6AAAAAAAAqSD0AAAAAAAAUkHoAQAAAAAApEK/Q48tW7bEggULIp/PRyaTiSeffLLs+pIlSyKTyZQdl19++WDVCwAAAAAAcFr9Dj2OHDkSF198caxfv/5j+1x99dXR2dlZOp599tlPVCQAAAAAAMCZjOnvDc3NzdHc3Nxrn2w2G7W1tQMuCgAAAAAAoL+GZE+PzZs3R3V1dVx44YWxdOnS2L9//8f2LRaLUSgUyg4AAAAAAID+GvTQo7m5OR599NF48cUX4wc/+EFs3749rrzyyigWi6ft39raGrlcrnTU19cPdkkAAAAAAMAokEmSJBnwzZlMbNq0KRYuXPixfTo7O6OhoSE2btwY1113XY/rxWKxLBApFAqCDwAAAAAAoEx3d3dUVlb22qffe3r0V11dXTQ0NMSePXtOez2bzUY2mx3qMgAAAAAAgJQbkj09PuzAgQOxb9++qKurG+qnAgAAAAAARrF+r/Q4fPhwvPnmm6Xzjo6OeP3116OqqiqqqqpizZo1cf3110ddXV289dZbcfvtt8eECRPi2muvHdTCAQAAAAAAPqzfocerr74a8+bNK52vXLkyIiIWL14c999/f7zxxhvx8MMPx/vvvx91dXUxb968ePzxx6OiomLwqgYAAAAAAPiIT7SR+VAoFAqRy+WGuwwAAAAAAGAE6ctG5kO+pwcAAAAAAMDZIPQAAAAAAABSQegBAAAAAACkgtADAAAAAABIBaEHAAAAAACQCkIPAAAAAAAgFYQeAAAAAABAKgg9AAAAAACAVBB6AAAAAAAAqSD0AAAAAAAAUkHoAQAAAAAApILQAwAAAAAASAWhBwAAAAAAkApCDwAAAAAAIBWEHgAAAAAAQCoIPQAAAAAAgFQQegAAAAAAAKkg9AAAAAAAAFJB6AEAAAAAAKSC0AMAAAAAAEgFoQcAAAAAAJAKQg8AAAAAACAVhB4AAAAAAEAqCD0AAAAAAIBUEHoAAAAAAACpIPQAAAAAAABSQegBAAAAAACkgtADAAAAAABIhX6FHq2trXHZZZdFRUVFVFdXx8KFC2P37t1lfZIkiTVr1kQ+n49x48bF3LlzY9euXYNaNAAAAAAAwEf1K/Rob2+PZcuWxbZt26KtrS0++OCDaGpqiiNHjpT63H333bFu3bpYv359bN++PWpra2P+/Plx6NChQS8eAAAAAADglEySJMlAb/7f//3fqK6ujvb29pg9e3YkSRL5fD5aWlpi9erVERFRLBajpqYm7rrrrrjlllvO+JiFQiFyudxASwIAAAAAAFKou7s7Kisre+3zifb06O7ujoiIqqqqiIjo6OiIrq6uaGpqKvXJZrMxZ86c2Lp162kfo1gsRqFQKDsAAAAAAAD6a8ChR5IksXLlyvjSl74UU6dOjYiIrq6uiIioqakp61tTU1O69lGtra2Ry+VKR319/UBLAgAAAAAARrEBhx7Lly+Pn/70p/Ev//IvPa5lMpmy8yRJerSdctttt0V3d3fp2Ldv30BLAgAAAAAARrExA7lpxYoV8fTTT8eWLVti4sSJpfba2tqI+O2Kj7q6ulL7/v37e6z+OCWbzUY2mx1IGQAAAAAAACX9WumRJEksX748nnjiiXjxxRejsbGx7HpjY2PU1tZGW1tbqe3YsWPR3t4es2bNGpyKAQAAAAAATqNfKz2WLVsWjz32WDz11FNRUVFR2qcjl8vFuHHjIpPJREtLS6xduzamTJkSU6ZMibVr18b48eNj0aJFQ/ICAAAAAAAAIiIySZIkfe78MftybNiwIZYsWRIRv10N8t3vfjceeOCBOHjwYMyYMSN++MMfljY7P5NCoRC5XK6vJQEAAAAAAKNAd3d3VFZW9tqnX6HH2SD0AAAAAAAAPqovoUe/9vQAAAAAAAAYqYQeAAAAAABAKgg9AAAAAACAVBB6AAAAAAAAqSD0AAAAAAAAUkHoAQAAAAAApILQAwAAAAAASAWhBwAAAAAAkApCDwAAAAAAIBWEHgAAAAAAQCoIPQAAAAAAgFQQegAAAAAAAKkg9AAAAAAAAFJB6AEAAAAAAKSC0AMAAAAAAEgFoQcAAAAAAJAKQg8AAAAAACAVhB4AAAAAAEAqCD0AAAAAAIBUEHoAAAAAAACpIPQAAAAAAABSQegBAAAAAACkgtADAAAAAABIBaEHAAAAAACQCkIPAAAAAAAgFYQeAAAAAABAKgg9AAAAAACAVBB6AAAAAAAAqdCv0KO1tTUuu+yyqKioiOrq6li4cGHs3r27rM+SJUsik8mUHZdffvmgFg0AAAAAAPBR/Qo92tvbY9myZbFt27Zoa2uLDz74IJqamuLIkSNl/a6++uro7OwsHc8+++ygFg0AAAAAAPBRY/rT+bnnnis737BhQ1RXV8eOHTti9uzZpfZsNhu1tbWDUyEAAAAAAEAffKI9Pbq7uyMioqqqqqx98+bNUV1dHRdeeGEsXbo09u/f/7GPUSwWo1AolB0AAAAAAAD9lUmSJBnIjUmSxDXXXBMHDx6Ml19+udT++OOPx2c/+9loaGiIjo6OuPPOO+ODDz6IHTt2RDab7fE4a9asie9+97sDfwUAAAAAAEDqdXd3R2VlZa99Bhx6LFu2LJ555pl45ZVXYuLEiR/br7OzMxoaGmLjxo1x3XXX9bheLBajWCyWzguFQtTX1w+kJAAAAAAAIKX6Enr0a0+PU1asWBFPP/10bNmypdfAIyKirq4uGhoaYs+ePae9ns1mT7sCBAAAAAAAoD/6FXokSRIrVqyITZs2xebNm6OxsfGM9xw4cCD27dsXdXV1Ay4SAAAAAADgTPq1kfmyZcvikUceicceeywqKiqiq6srurq64ujRoxERcfjw4Vi1alX8x3/8R7z11luxefPmWLBgQUyYMCGuvfbaIXkBAAAAAAAAEf3c0yOTyZy2fcOGDbFkyZI4evRoLFy4MF577bV4//33o66uLubNmxd///d/3+d9OgqFQuRyub6WBAAAAAAAjAJDupH5UBF6AAAAAAAAH9WX0KNfX28FAAAAAAAwUgk9AAAAAACAVBB6AAAAAAAAqSD0AAAAAAAAUkHoAQAAAAAApILQAwAAAAAASAWhBwAAAAAAkApCDwAAAAAAIBWEHgAAAAAAQCoIPQAAAAAAgFQQegAAAAAAAKkg9AAAAAAAAFJB6AEAAAAAAKSC0AMAAAAAAEgFoQcAAAAAAJAKQg8AAAAAACAVhB4AAAAAAEAqCD0AAAAAAIBUEHoAAAAAAACpIPQAAAAAAABSQegBAAAAAACkgtADAAAAAABIBaEHAAAAAACQCkIPAAAAAAAgFYQeAAAAAABAKgg9AAAAAACAVBB6AAAAAAAAqdCv0OP++++PL3zhC1FZWRmVlZUxc+bM+Pd///fS9SRJYs2aNZHP52PcuHExd+7c2LVr16AXDQAAAAAA8FH9Cj0mTpwY3//+9+PVV1+NV199Na688sq45pprSsHG3XffHevWrYv169fH9u3bo7a2NubPnx+HDh0akuIBAAAAAABOySRJknySB6iqqop77rknvvGNb0Q+n4+WlpZYvXp1REQUi8WoqamJu+66K2655ZY+PV6hUIhcLvdJSgIAAAAAAFKmu7s7Kisre+0z4D09Tpw4ERs3bowjR47EzJkzo6OjI7q6uqKpqanUJ5vNxpw5c2Lr1q0DfRoAAAAAAIA+GdPfG954442YOXNm/OY3v4nPfvazsWnTpvjDP/zDUrBRU1NT1r+mpibefvvtj328YrEYxWKxdF4oFPpbEgAAAAAAQP9Xevz+7/9+vP7667Ft27b467/+61i8eHH8/Oc/L13PZDJl/ZMk6dH2Ya2trZHL5UpHfX19f0sCAAAAAAD45Ht6XHXVVTF58uRYvXp1TJ48OXbu3Blf/OIXS9evueaa+NznPhcPPfTQae8/3UoPwQcAAAAAAPBhQ7qnxylJkkSxWIzGxsaora2Ntra20rVjx45Fe3t7zJo162Pvz2azUVlZWXYAAAAAAAD0V7/29Lj99tujubk56uvr49ChQ7Fx48bYvHlzPPfcc5HJZKKlpSXWrl0bU6ZMiSlTpsTatWtj/PjxsWjRoqGqHwAAAAAAICL6GXq8++67cdNNN0VnZ2fkcrn4whe+EM8991zMnz8/IiJuvfXWOHr0aHzrW9+KgwcPxowZM+InP/lJVFRUDEnxAAAAAAAAp3ziPT0GW6FQiFwuN9xlAAAAAAAAI8hZ2dMDAAAAAABgJBB6AAAAAAAAqSD0AAAAAAAAUkHoAQAAAAAApMKICz1G2L7qAAAAAADACNCX/GDEhR6HDh0a7hIAAAAAAIARpi/5QSYZYUsrTp48Ge+8805UVFREJpOJQqEQ9fX1sW/fvqisrBzu8mBYGAdgHECEcQDGABgHEGEcQIRxwOiTJEkcOnQo8vl8nHNO72s5xpylmvrsnHPOiYkTJ/Zor6ysNIAZ9YwDMA4gwjgAYwCMA4gwDiDCOGB0yeVyfeo34r7eCgAAAAAAYCCEHgAAAAAAQCqM+NAjm83Gd77znchms8NdCgwb4wCMA4gwDsAYAOMAIowDiDAOoDcjbiNzAAAAAACAgRjxKz0AAAAAAAD6QugBAAAAAACkgtADAAAAAABIBaEHAAAAAACQCiM+9LjvvvuisbExPvOZz8T06dPj5ZdfHu6SYEi0trbGZZddFhUVFVFdXR0LFy6M3bt3l/VZsmRJZDKZsuPyyy8fpoph8K1Zs6bHz3htbW3pepIksWbNmsjn8zFu3LiYO3du7Nq1axgrhsH3u7/7uz3GQSaTiWXLlkWEuYB02rJlSyxYsCDy+XxkMpl48skny6735f2/WCzGihUrYsKECXH++efHV77ylfjVr351Fl8FDFxvY+D48eOxevXqmDZtWpx//vmRz+fjL//yL+Odd94pe4y5c+f2mB9uvPHGs/xKYODONBf05TOQuYBPuzONg9P9npDJZOKee+4p9TEfwAgPPR5//PFoaWmJO+64I1577bW44oororm5Ofbu3TvcpcGga29vj2XLlsW2bduira0tPvjgg2hqaoojR46U9bv66qujs7OzdDz77LPDVDEMjYsuuqjsZ/yNN94oXbv77rtj3bp1sX79+ti+fXvU1tbG/Pnz49ChQ8NYMQyu7du3l42Btra2iIj42te+VupjLiBtjhw5EhdffHGsX7/+tNf78v7f0tISmzZtio0bN8Yrr7wShw8fji9/+ctx4sSJs/UyYMB6GwO//vWvY+fOnXHnnXfGzp0744knnoj//u//jq985Ss9+i5durRsfnjggQfORvkwKM40F0Sc+TOQuYBPuzONgw///Hd2dsaPf/zjyGQycf3115f1Mx8w2o0Z7gJ6s27durj55pvjr/7qryIi4t57743nn38+7r///mhtbR3m6mBwPffcc2XnGzZsiOrq6tixY0fMnj271J7NZsv+8h3SZsyYMaf9GU+SJO69996444474rrrrouIiIceeihqamrisccei1tuueVslwpD4oILLig7//73vx+TJ0+OOXPmlNrMBaRNc3NzNDc3n/ZaX97/u7u748EHH4x//ud/jquuuioiIh555JGor6+PF154If7kT/7krL0WGIjexkAulysF4Kf8wz/8Q/zRH/1R7N27NyZNmlRqHz9+vPmBT63exsEpvX0GMheQBmcaBx/9+X/qqadi3rx58Xu/93tl7eYDRrsRu9Lj2LFjsWPHjmhqaiprb2pqiq1btw5TVXD2dHd3R0REVVVVWfvmzZujuro6Lrzwwli6dGns379/OMqDIbNnz57I5/PR2NgYN954Y/zyl7+MiIiOjo7o6uoqmxey2WzMmTPHvEBqHTt2LB555JH4xje+EZlMptRuLmA06cv7/44dO+L48eNlffL5fEydOtUcQSp1d3dHJpOJz33uc2Xtjz76aEyYMCEuuuiiWLVqldWwpE5vn4HMBYw27777bjzzzDNx880397hmPmC0G7ErPd577704ceJE1NTUlLXX1NREV1fXMFUFZ0eSJLFy5cr40pe+FFOnTi21Nzc3x9e+9rVoaGiIjo6OuPPOO+PKK6+MHTt2RDabHcaKYXDMmDEjHn744bjwwgvj3Xffje9973sxa9as2LVrV+m9/3Tzwttvvz0c5cKQe/LJJ+P999+PJUuWlNrMBYw2fXn/7+rqirFjx8bv/M7v9OjjdwfS5je/+U387d/+bSxatCgqKytL7V//+tejsbExamtr42c/+1ncdttt8V//9V89VonAp9WZPgOZCxhtHnrooaioqCithD3FfAAjOPQ45cN/1Rjx2/8M/mgbpM3y5cvjpz/9abzyyitl7TfccEPp31OnTo1LL700Ghoa4plnnukxycGn0YeX8U6bNi1mzpwZkydPjoceeqi0SaF5gdHkwQcfjObm5sjn86U2cwGj1UDe/80RpM3x48fjxhtvjJMnT8Z9991Xdm3p0qWlf0+dOjWmTJkSl156aezcuTMuueSSs10qDLqBfgYyF5BWP/7xj+PrX/96fOYznylrNx/ACP56qwkTJsS5557bI43fv39/j7/ygjRZsWJFPP300/HSSy/FxIkTe+1bV1cXDQ0NsWfPnrNUHZxd559/fkybNi327NlT+j5S8wKjxdtvvx0vvPBCaW+zj2MuIO368v5fW1sbx44di4MHD35sH/i0O378ePz5n/95dHR0RFtbW9kqj9O55JJL4rzzzjM/kFof/QxkLmA0efnll2P37t1n/F0hwnzA6DRiQ4+xY8fG9OnTeyy9amtri1mzZg1TVTB0kiSJ5cuXxxNPPBEvvvhiNDY2nvGeAwcOxL59+6Kuru4sVAhnX7FYjF/84hdRV1dXWp774Xnh2LFj0d7ebl4glTZs2BDV1dXxZ3/2Z732MxeQdn15/58+fXqcd955ZX06OzvjZz/7mTmCVDgVeOzZsydeeOGF+PznP3/Ge3bt2hXHjx83P5BaH/0MZC5gNHnwwQdj+vTpcfHFF5+xr/mA0WhEf73VypUr46abbopLL700Zs6cGT/60Y9i79698c1vfnO4S4NBt2zZsnjsscfiqaeeioqKitJfM+ZyuRg3blwcPnw41qxZE9dff33U1dXFW2+9FbfffntMmDAhrr322mGuHgbHqlWrYsGCBTFp0qTYv39/fO9734tCoRCLFy+OTCYTLS0tsXbt2pgyZUpMmTIl1q5dG+PHj49FixYNd+kwqE6ePBkbNmyIxYsXx5gx//dxzVxAWh0+fDjefPPN0nlHR0e8/vrrUVVVFZMmTTrj+38ul4ubb745vv3tb8fnP//5qKqqilWrVsW0adPiqquuGq6XBX3W2xjI5/Px1a9+NXbu3Bn/9m//FidOnCj9rlBVVRVjx46N//mf/4lHH300/vRP/zQmTJgQP//5z+Pb3/52fPGLX4w//uM/Hq6XBf3S2zioqqo642cgcwFpcKbPRBERhUIh/vVf/zV+8IMf9LjffAD/XzLC/fCHP0waGhqSsWPHJpdccknS3t4+3CXBkIiI0x4bNmxIkiRJfv3rXydNTU3JBRdckJx33nnJpEmTksWLFyd79+4d3sJhEN1www1JXV1dct555yX5fD657rrrkl27dpWunzx5MvnOd76T1NbWJtlsNpk9e3byxhtvDGPFMDSef/75JCKS3bt3l7WbC0irl1566bSfgxYvXpwkSd/e/48ePZosX748qaqqSsaNG5d8+ctfNjb41OhtDHR0dHzs7wovvfRSkiRJsnfv3mT27NlJVVVVMnbs2GTy5MnJ3/zN3yQHDhwY3hcG/dDbOOjrZyBzAZ92Z/pMlCRJ8sADDyTjxo1L3n///R73mw/gtzJJkiRDnqwAAAAAAAAMsRG7pwcAAAAAAEB/CD0AAAAAAIBUEHoAAAAAAACpIPQAAAAAAABSQegBAAAAAACkgtADAAAAAABIBaEHAAAAAACQCkIPAAAAAAAgFYQeAAAAAABAKgg9AAAAAACAVBB6AAAAAAAAqSD0AAAAAAAAUuH/AVpA9TVEJgv9AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 2000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# here we can visualize the number of activations that will set the resultant gradient to 0 in a backward pass (white squares)\n",
    "# note that each row shows the 200 weights for each sample. If a column is all white, you have a dead neuron!\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.imshow(h.abs() > 0.99, cmap=\"gray\", interpolation=\"nearest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "b38a05e2-98ee-41a5-aed7-1195a554a74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # calibrate the batch norm at end of training\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     # pass training set through\n",
    "#     emb = C[Xtr]\n",
    "#     embcat = emb.view(emb.shape[0], -1)\n",
    "#     hpreact = embcat @ W1 + b1\n",
    "#     #measure mean and std dev over entire training set\n",
    "#     bnmean = hpreact.mean(0, keepdim=True)\n",
    "#     bnstd = hpreact.std(0, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "26635583-7fdd-4cdf-b4cf-ca79ecf09f25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 2.1040475368499756\n",
      "val 2.1498501300811768\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def split_loss(split):\n",
    "    x, y = {\n",
    "        'train': (Xtr, Ytr),\n",
    "        'val': (Xdev, Ydev),\n",
    "        'test': (Xte, Yte),\n",
    "    }[split]\n",
    "    emb = C[x] # (test rows, block_size ) indexing a (word_size, n_embed) => (n_rows, block_size, n_embed) )\n",
    "    embcat = emb.view(emb.shape[0], -1) # (n_rows, embed_size * block_size * n_embed)a\n",
    "    hpreact = embcat @ W1 + b1\n",
    "    hpreact =  bngain * ((hpreact - bnmean_running)/ bnstd_running) + bnbias ##using batch norm over entire training set vs. those calc'd during training\n",
    "    h = torch.tanh( hpreact) # (word-grams, 100)\n",
    "    logits = h @ W2 + b2\n",
    "    loss = F.cross_entropy(logits, y)\n",
    "    print(split, loss.item())\n",
    "\n",
    "split_loss('train')\n",
    "split_loss('val')\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "2d60344a-a812-4e1a-875c-c6ab5c0d0f1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      "z.\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      "b.\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      "z.\n",
      ".\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "for _ in range(20):\n",
    "    out = []\n",
    "    context = [0] * block_size #(3)\n",
    "    while True:\n",
    "        emb = C[torch.tensor([context])] # (1, 3) indexing a (27, 2) => (1, 3, 2)\n",
    "        h = torch.tanh(emb.view(1, -1) @ W1 + b1) # (1, 6) @ (6, 100)\n",
    "        logits = h @ W2 + b2\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
    "        context = context[1:] + [ix]\n",
    "        out.append(ix)\n",
    "        if ix == 0:\n",
    "            break\n",
    "    print(''.join(itos[i] for i in out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "b20adcd5-904c-4558-b84d-69582afd4052",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Pytorchify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "64c05a91-d4e4-4236-91fd-889acb98b244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47024\n"
     ]
    }
   ],
   "source": [
    "class Linear:\n",
    "    def __init__(self, fan_in, fan_out, bias=True):\n",
    "        self.weight = torch.randn(fan_in, fan_out, generator=g) /fan_in**0.5 #kaiming initilization\n",
    "        self.bias = torch.zeros(fan_out) if bias else None\n",
    "\n",
    "    def __call__(self, x):\n",
    "        self.out = x @ self.weight\n",
    "        if self.bias is not None:\n",
    "            self.out += self.bias\n",
    "        return self.out\n",
    "        \n",
    "    def parameters(self):\n",
    "        return [self.weight] + ([] if self.bias is None else [self.bias])\n",
    "\n",
    "\n",
    "class BatchNorm1d:\n",
    "    def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "        self.training = True\n",
    "        self.gamma = torch.ones(dim)\n",
    "        self.beta = torch.zeros(dim)\n",
    "        #running val buffers\n",
    "        self.running_mean = torch.zeros(dim)\n",
    "        self.running_var = torch.ones(dim)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        if self.training:\n",
    "            xmean = x.mean(0, keepdim=True)\n",
    "            xvar = x.var(0, keepdim=True)\n",
    "        else:\n",
    "            xmean = self.running_mean\n",
    "            xvar = self.running_var\n",
    "\n",
    "        xhat =  (x - xmean) / torch.sqrt(xvar + self.eps)\n",
    "        self.out = self.gamma * xhat + self.beta\n",
    "        \n",
    "        if self.training:\n",
    "            with torch.no_grad():\n",
    "                self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * xmean\n",
    "                self.running_var = (1 - self.momentum) * self.running_var + self.momentum * xvar\n",
    "        \n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.gamma, self.beta]\n",
    "\n",
    "\n",
    "class Tanh:\n",
    "    def __call__(self, x):\n",
    "        self.out = torch.tanh(x)\n",
    "        return self.out\n",
    "    def parameters(self):\n",
    "        return []\n",
    "\n",
    "\n",
    "n_embd = 10 # the dimensionality of the character embedding vectors\n",
    "n_hidden = 100 # the number of neurons in the hidden layer of the MLP\n",
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "\n",
    "C = torch.randn((vocab_size, n_embed), generator=g)\n",
    "\n",
    "layers = [\n",
    "    Linear(n_embd * block_size, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
    "    Linear(n_hidden, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
    "    Linear(n_hidden, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
    "    Linear(n_hidden, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
    "    Linear(n_hidden, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
    "    Linear(n_hidden, vocab_size, bias=False), BatchNorm1d(vocab_size),\n",
    "]\n",
    "\n",
    "#set gains/ adjust final layer confidence etc.\n",
    "with torch.no_grad():\n",
    "    layers[-1].gamma *= 0.1\n",
    "    # all other layers apply gain\n",
    "    for layer in layers[:-1]:\n",
    "        if isinstance(layer, Linear):\n",
    "            l.weight *= (5/3)\n",
    "\n",
    "parameters = [C] + [p for layer in layers for p in layer.parameters()]\n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n",
    "\n",
    "print(sum(p.nelement() for p in parameters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "6d1cfaf4-2576-4104-97a5-c68f05be1fd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      1/ 200000: 2.3242\n",
      "      2/ 200000: 2.5870\n",
      "      3/ 200000: 2.4752\n",
      "      4/ 200000: 2.2055\n",
      "      5/ 200000: 2.3800\n",
      "      6/ 200000: 2.5453\n",
      "      7/ 200000: 2.3064\n",
      "      8/ 200000: 2.4881\n",
      "      9/ 200000: 2.4268\n",
      "     10/ 200000: 2.3051\n",
      "     11/ 200000: 2.1655\n",
      "     12/ 200000: 2.5223\n",
      "     13/ 200000: 2.4882\n",
      "     14/ 200000: 2.5424\n",
      "     15/ 200000: 2.2907\n",
      "     16/ 200000: 2.4918\n",
      "     17/ 200000: 2.5144\n",
      "     18/ 200000: 2.3878\n",
      "     19/ 200000: 2.6074\n",
      "     20/ 200000: 2.6051\n",
      "     21/ 200000: 2.7224\n",
      "     22/ 200000: 2.1327\n",
      "     23/ 200000: 2.8272\n",
      "     24/ 200000: 2.7429\n",
      "     25/ 200000: 2.6055\n",
      "     26/ 200000: 2.7853\n",
      "     27/ 200000: 2.3601\n",
      "     28/ 200000: 2.7218\n",
      "     29/ 200000: 2.1697\n",
      "     30/ 200000: 2.7524\n",
      "     31/ 200000: 2.6130\n",
      "     32/ 200000: 2.2344\n",
      "     33/ 200000: 2.0939\n",
      "     34/ 200000: 2.6527\n",
      "     35/ 200000: 2.3067\n",
      "     36/ 200000: 2.5536\n",
      "     37/ 200000: 2.6156\n",
      "     38/ 200000: 2.4037\n",
      "     39/ 200000: 3.0598\n",
      "     40/ 200000: 2.5727\n",
      "     41/ 200000: 2.5843\n",
      "     42/ 200000: 2.4500\n",
      "     43/ 200000: 2.2494\n",
      "     44/ 200000: 2.4697\n",
      "     45/ 200000: 2.3926\n",
      "     46/ 200000: 2.2607\n",
      "     47/ 200000: 2.5717\n",
      "     48/ 200000: 1.9865\n",
      "     49/ 200000: 2.1739\n",
      "     50/ 200000: 2.5139\n",
      "     51/ 200000: 2.3930\n",
      "     52/ 200000: 3.0199\n",
      "     53/ 200000: 2.2566\n",
      "     54/ 200000: 2.4610\n",
      "     55/ 200000: 2.5121\n",
      "     56/ 200000: 2.3828\n",
      "     57/ 200000: 2.4833\n",
      "     58/ 200000: 2.6383\n",
      "     59/ 200000: 2.2794\n",
      "     60/ 200000: 2.4778\n",
      "     61/ 200000: 2.5497\n",
      "     62/ 200000: 2.1659\n",
      "     63/ 200000: 2.3968\n",
      "     64/ 200000: 2.3685\n",
      "     65/ 200000: 2.2615\n",
      "     66/ 200000: 2.5917\n",
      "     67/ 200000: 2.0786\n",
      "     68/ 200000: 2.2219\n",
      "     69/ 200000: 2.5601\n",
      "     70/ 200000: 2.8645\n",
      "     71/ 200000: 2.6792\n",
      "     72/ 200000: 2.4869\n",
      "     73/ 200000: 2.3096\n",
      "     74/ 200000: 2.5105\n",
      "     75/ 200000: 2.6713\n",
      "     76/ 200000: 2.5637\n",
      "     77/ 200000: 2.5325\n",
      "     78/ 200000: 2.4522\n",
      "     79/ 200000: 2.5158\n",
      "     80/ 200000: 2.1465\n",
      "     81/ 200000: 2.1498\n",
      "     82/ 200000: 2.4350\n",
      "     83/ 200000: 2.8320\n",
      "     84/ 200000: 2.7130\n",
      "     85/ 200000: 2.2449\n",
      "     86/ 200000: 2.3077\n",
      "     87/ 200000: 2.3346\n",
      "     88/ 200000: 2.4012\n",
      "     89/ 200000: 2.2496\n",
      "     90/ 200000: 2.6507\n",
      "     91/ 200000: 2.5375\n",
      "     92/ 200000: 2.5188\n",
      "     93/ 200000: 2.2857\n",
      "     94/ 200000: 2.5880\n",
      "     95/ 200000: 2.6104\n",
      "     96/ 200000: 2.6980\n",
      "     97/ 200000: 2.3482\n",
      "     98/ 200000: 2.3057\n",
      "     99/ 200000: 2.3158\n",
      "    100/ 200000: 2.6940\n",
      "    101/ 200000: 2.4938\n",
      "    102/ 200000: 2.4083\n",
      "    103/ 200000: 2.4545\n",
      "    104/ 200000: 2.3534\n",
      "    105/ 200000: 2.4327\n",
      "    106/ 200000: 1.9857\n",
      "    107/ 200000: 2.0569\n",
      "    108/ 200000: 2.2698\n",
      "    109/ 200000: 2.6841\n",
      "    110/ 200000: 2.4135\n",
      "    111/ 200000: 2.3093\n",
      "    112/ 200000: 2.6832\n",
      "    113/ 200000: 2.2707\n",
      "    114/ 200000: 2.2741\n",
      "    115/ 200000: 2.3420\n",
      "    116/ 200000: 2.8142\n",
      "    117/ 200000: 2.7520\n",
      "    118/ 200000: 2.2337\n",
      "    119/ 200000: 2.1864\n",
      "    120/ 200000: 2.7708\n",
      "    121/ 200000: 2.4934\n",
      "    122/ 200000: 2.4844\n",
      "    123/ 200000: 2.1031\n",
      "    124/ 200000: 2.4542\n",
      "    125/ 200000: 2.7001\n",
      "    126/ 200000: 2.5081\n",
      "    127/ 200000: 2.3248\n",
      "    128/ 200000: 2.2919\n",
      "    129/ 200000: 2.3762\n",
      "    130/ 200000: 2.2845\n",
      "    131/ 200000: 2.4049\n",
      "    132/ 200000: 2.3225\n",
      "    133/ 200000: 2.2096\n",
      "    134/ 200000: 2.6721\n",
      "    135/ 200000: 2.4010\n",
      "    136/ 200000: 2.5817\n",
      "    137/ 200000: 2.6200\n",
      "    138/ 200000: 2.4666\n",
      "    139/ 200000: 2.5439\n",
      "    140/ 200000: 2.5799\n",
      "    141/ 200000: 2.3762\n",
      "    142/ 200000: 2.4073\n",
      "    143/ 200000: 2.5367\n",
      "    144/ 200000: 2.9609\n",
      "    145/ 200000: 2.4384\n",
      "    146/ 200000: 2.3317\n",
      "    147/ 200000: 2.5715\n",
      "    148/ 200000: 2.5258\n",
      "    149/ 200000: 2.4988\n",
      "    150/ 200000: 2.5383\n",
      "    151/ 200000: 2.3860\n",
      "    152/ 200000: 2.8186\n",
      "    153/ 200000: 2.6945\n",
      "    154/ 200000: 2.6600\n",
      "    155/ 200000: 2.3029\n",
      "    156/ 200000: 2.2902\n",
      "    157/ 200000: 2.3803\n",
      "    158/ 200000: 2.1209\n",
      "    159/ 200000: 2.5666\n",
      "    160/ 200000: 2.3459\n",
      "    161/ 200000: 2.6458\n",
      "    162/ 200000: 2.7590\n",
      "    163/ 200000: 2.3800\n",
      "    164/ 200000: 2.4646\n",
      "    165/ 200000: 2.3417\n",
      "    166/ 200000: 2.4816\n",
      "    167/ 200000: 2.7379\n",
      "    168/ 200000: 2.6583\n",
      "    169/ 200000: 2.2421\n",
      "    170/ 200000: 2.4323\n",
      "    171/ 200000: 2.5758\n",
      "    172/ 200000: 2.7824\n",
      "    173/ 200000: 2.4806\n",
      "    174/ 200000: 2.5964\n",
      "    175/ 200000: 2.5697\n",
      "    176/ 200000: 2.2082\n",
      "    177/ 200000: 2.8510\n",
      "    178/ 200000: 2.4533\n",
      "    179/ 200000: 2.4424\n",
      "    180/ 200000: 2.7430\n",
      "    181/ 200000: 2.4032\n",
      "    182/ 200000: 2.5140\n",
      "    183/ 200000: 2.8760\n",
      "    184/ 200000: 2.5729\n",
      "    185/ 200000: 2.4775\n",
      "    186/ 200000: 2.7964\n",
      "    187/ 200000: 2.4870\n",
      "    188/ 200000: 1.9212\n",
      "    189/ 200000: 2.3648\n",
      "    190/ 200000: 2.3419\n",
      "    191/ 200000: 2.0214\n",
      "    192/ 200000: 2.4420\n",
      "    193/ 200000: 2.6207\n",
      "    194/ 200000: 2.6835\n",
      "    195/ 200000: 2.8493\n",
      "    196/ 200000: 2.5733\n",
      "    197/ 200000: 2.3604\n",
      "    198/ 200000: 2.3690\n",
      "    199/ 200000: 2.4883\n",
      "    200/ 200000: 2.4372\n",
      "    201/ 200000: 2.6952\n",
      "    202/ 200000: 2.2783\n",
      "    203/ 200000: 2.4304\n",
      "    204/ 200000: 2.4332\n",
      "    205/ 200000: 2.6189\n",
      "    206/ 200000: 2.5917\n",
      "    207/ 200000: 2.6566\n",
      "    208/ 200000: 2.2827\n",
      "    209/ 200000: 2.5754\n",
      "    210/ 200000: 2.4179\n",
      "    211/ 200000: 2.3127\n",
      "    212/ 200000: 2.0696\n",
      "    213/ 200000: 2.2516\n",
      "    214/ 200000: 2.6404\n",
      "    215/ 200000: 2.3450\n",
      "    216/ 200000: 2.5894\n",
      "    217/ 200000: 2.8536\n",
      "    218/ 200000: 2.5217\n",
      "    219/ 200000: 2.3089\n",
      "    220/ 200000: 2.4161\n",
      "    221/ 200000: 2.3901\n",
      "    222/ 200000: 2.3797\n",
      "    223/ 200000: 2.5692\n",
      "    224/ 200000: 2.3514\n",
      "    225/ 200000: 2.5100\n",
      "    226/ 200000: 2.5851\n",
      "    227/ 200000: 2.3778\n",
      "    228/ 200000: 2.5966\n",
      "    229/ 200000: 2.4429\n",
      "    230/ 200000: 2.5229\n",
      "    231/ 200000: 2.8106\n",
      "    232/ 200000: 2.6704\n",
      "    233/ 200000: 2.4845\n",
      "    234/ 200000: 2.5407\n",
      "    235/ 200000: 2.3514\n",
      "    236/ 200000: 2.6942\n",
      "    237/ 200000: 2.5870\n",
      "    238/ 200000: 2.0612\n",
      "    239/ 200000: 2.3699\n",
      "    240/ 200000: 2.5886\n",
      "    241/ 200000: 2.9085\n",
      "    242/ 200000: 2.5395\n",
      "    243/ 200000: 2.6395\n",
      "    244/ 200000: 2.4141\n",
      "    245/ 200000: 2.5150\n",
      "    246/ 200000: 2.4103\n",
      "    247/ 200000: 2.5555\n",
      "    248/ 200000: 2.4555\n",
      "    249/ 200000: 2.5057\n",
      "    250/ 200000: 2.5343\n",
      "    251/ 200000: 2.4226\n",
      "    252/ 200000: 2.2798\n",
      "    253/ 200000: 2.4554\n",
      "    254/ 200000: 2.2803\n",
      "    255/ 200000: 2.3987\n",
      "    256/ 200000: 2.3804\n",
      "    257/ 200000: 2.3396\n",
      "    258/ 200000: 2.3362\n",
      "    259/ 200000: 2.2999\n",
      "    260/ 200000: 2.2782\n",
      "    261/ 200000: 2.2302\n",
      "    262/ 200000: 2.2810\n",
      "    263/ 200000: 2.5491\n",
      "    264/ 200000: 2.3796\n",
      "    265/ 200000: 2.5606\n",
      "    266/ 200000: 2.3443\n",
      "    267/ 200000: 2.1449\n",
      "    268/ 200000: 2.0606\n",
      "    269/ 200000: 2.7573\n",
      "    270/ 200000: 2.3561\n",
      "    271/ 200000: 2.0535\n",
      "    272/ 200000: 2.6759\n",
      "    273/ 200000: 2.3260\n",
      "    274/ 200000: 2.4757\n",
      "    275/ 200000: 2.5558\n",
      "    276/ 200000: 2.4380\n",
      "    277/ 200000: 2.6607\n",
      "    278/ 200000: 1.9394\n",
      "    279/ 200000: 2.5026\n",
      "    280/ 200000: 2.6982\n",
      "    281/ 200000: 2.2490\n",
      "    282/ 200000: 2.3917\n",
      "    283/ 200000: 2.5592\n",
      "    284/ 200000: 2.3123\n",
      "    285/ 200000: 2.4438\n",
      "    286/ 200000: 2.1877\n",
      "    287/ 200000: 2.4874\n",
      "    288/ 200000: 2.2804\n",
      "    289/ 200000: 2.6601\n",
      "    290/ 200000: 2.6604\n",
      "    291/ 200000: 2.2884\n",
      "    292/ 200000: 2.5421\n",
      "    293/ 200000: 2.5269\n",
      "    294/ 200000: 2.3986\n",
      "    295/ 200000: 2.2650\n",
      "    296/ 200000: 2.3397\n",
      "    297/ 200000: 2.3433\n",
      "    298/ 200000: 2.2936\n",
      "    299/ 200000: 2.5050\n",
      "    300/ 200000: 2.3702\n",
      "    301/ 200000: 2.2416\n",
      "    302/ 200000: 2.5700\n",
      "    303/ 200000: 2.2719\n",
      "    304/ 200000: 2.7054\n",
      "    305/ 200000: 2.7741\n",
      "    306/ 200000: 2.2688\n",
      "    307/ 200000: 2.5470\n",
      "    308/ 200000: 2.4509\n",
      "    309/ 200000: 2.5025\n",
      "    310/ 200000: 2.7394\n",
      "    311/ 200000: 2.7098\n",
      "    312/ 200000: 2.5063\n",
      "    313/ 200000: 2.3728\n",
      "    314/ 200000: 2.1586\n",
      "    315/ 200000: 2.6579\n",
      "    316/ 200000: 2.3820\n",
      "    317/ 200000: 2.3320\n",
      "    318/ 200000: 2.4183\n",
      "    319/ 200000: 2.3577\n",
      "    320/ 200000: 2.4939\n",
      "    321/ 200000: 2.2871\n",
      "    322/ 200000: 2.4522\n",
      "    323/ 200000: 2.3900\n",
      "    324/ 200000: 2.5654\n",
      "    325/ 200000: 2.3572\n",
      "    326/ 200000: 2.3164\n",
      "    327/ 200000: 2.3179\n",
      "    328/ 200000: 2.4345\n",
      "    329/ 200000: 2.4027\n",
      "    330/ 200000: 2.1282\n",
      "    331/ 200000: 2.2518\n",
      "    332/ 200000: 2.3642\n",
      "    333/ 200000: 2.5924\n",
      "    334/ 200000: 2.2688\n",
      "    335/ 200000: 2.3742\n",
      "    336/ 200000: 2.5886\n",
      "    337/ 200000: 2.4714\n",
      "    338/ 200000: 2.4572\n",
      "    339/ 200000: 2.6052\n",
      "    340/ 200000: 2.2177\n",
      "    341/ 200000: 2.6129\n",
      "    342/ 200000: 2.0741\n",
      "    343/ 200000: 2.1870\n",
      "    344/ 200000: 2.4193\n",
      "    345/ 200000: 2.3193\n",
      "    346/ 200000: 2.3618\n",
      "    347/ 200000: 2.5729\n",
      "    348/ 200000: 2.4396\n",
      "    349/ 200000: 2.6535\n",
      "    350/ 200000: 2.6468\n",
      "    351/ 200000: 2.3757\n",
      "    352/ 200000: 2.1681\n",
      "    353/ 200000: 2.2699\n",
      "    354/ 200000: 2.3217\n",
      "    355/ 200000: 2.4536\n",
      "    356/ 200000: 2.4246\n",
      "    357/ 200000: 2.3145\n",
      "    358/ 200000: 2.5316\n",
      "    359/ 200000: 2.4694\n",
      "    360/ 200000: 2.3109\n",
      "    361/ 200000: 2.5193\n",
      "    362/ 200000: 2.3688\n",
      "    363/ 200000: 2.6632\n",
      "    364/ 200000: 2.1008\n",
      "    365/ 200000: 2.1484\n",
      "    366/ 200000: 2.6567\n",
      "    367/ 200000: 2.7717\n",
      "    368/ 200000: 2.3326\n",
      "    369/ 200000: 2.5052\n",
      "    370/ 200000: 2.1912\n",
      "    371/ 200000: 2.7771\n",
      "    372/ 200000: 2.0922\n",
      "    373/ 200000: 2.7440\n",
      "    374/ 200000: 2.1274\n",
      "    375/ 200000: 2.2433\n",
      "    376/ 200000: 2.4556\n",
      "    377/ 200000: 2.5428\n",
      "    378/ 200000: 2.3286\n",
      "    379/ 200000: 2.8732\n",
      "    380/ 200000: 2.5600\n",
      "    381/ 200000: 2.2880\n",
      "    382/ 200000: 2.4528\n",
      "    383/ 200000: 2.1977\n",
      "    384/ 200000: 2.3062\n",
      "    385/ 200000: 2.5362\n",
      "    386/ 200000: 2.0717\n",
      "    387/ 200000: 2.8003\n",
      "    388/ 200000: 2.6121\n",
      "    389/ 200000: 2.4363\n",
      "    390/ 200000: 2.2411\n",
      "    391/ 200000: 2.5423\n",
      "    392/ 200000: 2.4579\n",
      "    393/ 200000: 2.4847\n",
      "    394/ 200000: 2.8330\n",
      "    395/ 200000: 2.5968\n",
      "    396/ 200000: 2.4222\n",
      "    397/ 200000: 2.7539\n",
      "    398/ 200000: 2.2189\n",
      "    399/ 200000: 2.2730\n",
      "    400/ 200000: 2.0200\n",
      "    401/ 200000: 1.9617\n",
      "    402/ 200000: 2.2907\n",
      "    403/ 200000: 2.3870\n",
      "    404/ 200000: 2.3882\n",
      "    405/ 200000: 2.4501\n",
      "    406/ 200000: 2.4663\n",
      "    407/ 200000: 2.3085\n",
      "    408/ 200000: 2.4461\n",
      "    409/ 200000: 2.7589\n",
      "    410/ 200000: 2.6925\n",
      "    411/ 200000: 2.5047\n",
      "    412/ 200000: 2.4999\n",
      "    413/ 200000: 2.3125\n",
      "    414/ 200000: 2.1919\n",
      "    415/ 200000: 2.2587\n",
      "    416/ 200000: 2.1586\n",
      "    417/ 200000: 2.6034\n",
      "    418/ 200000: 2.2115\n",
      "    419/ 200000: 2.6960\n",
      "    420/ 200000: 2.3975\n",
      "    421/ 200000: 2.2547\n",
      "    422/ 200000: 2.4302\n",
      "    423/ 200000: 2.4748\n",
      "    424/ 200000: 2.2918\n",
      "    425/ 200000: 2.2212\n",
      "    426/ 200000: 2.5019\n",
      "    427/ 200000: 2.4101\n",
      "    428/ 200000: 2.5516\n",
      "    429/ 200000: 2.6560\n",
      "    430/ 200000: 2.3343\n",
      "    431/ 200000: 2.3359\n",
      "    432/ 200000: 2.4507\n",
      "    433/ 200000: 2.5099\n",
      "    434/ 200000: 2.5164\n",
      "    435/ 200000: 2.0986\n",
      "    436/ 200000: 2.4131\n",
      "    437/ 200000: 2.3197\n",
      "    438/ 200000: 2.5212\n",
      "    439/ 200000: 2.3963\n",
      "    440/ 200000: 2.4101\n",
      "    441/ 200000: 2.5485\n",
      "    442/ 200000: 2.4610\n",
      "    443/ 200000: 2.4921\n",
      "    444/ 200000: 2.6612\n",
      "    445/ 200000: 2.6245\n",
      "    446/ 200000: 2.4023\n",
      "    447/ 200000: 2.3193\n",
      "    448/ 200000: 2.3508\n",
      "    449/ 200000: 1.9850\n",
      "    450/ 200000: 2.2458\n",
      "    451/ 200000: 2.4574\n",
      "    452/ 200000: 2.7591\n",
      "    453/ 200000: 2.2118\n",
      "    454/ 200000: 2.3470\n",
      "    455/ 200000: 2.2832\n",
      "    456/ 200000: 2.2658\n",
      "    457/ 200000: 2.5450\n",
      "    458/ 200000: 2.4276\n",
      "    459/ 200000: 2.4924\n",
      "    460/ 200000: 2.6275\n",
      "    461/ 200000: 2.3354\n",
      "    462/ 200000: 2.2908\n",
      "    463/ 200000: 2.3761\n",
      "    464/ 200000: 2.3679\n",
      "    465/ 200000: 2.1835\n",
      "    466/ 200000: 2.1905\n",
      "    467/ 200000: 2.2605\n",
      "    468/ 200000: 2.5179\n",
      "    469/ 200000: 2.4614\n",
      "    470/ 200000: 2.3054\n",
      "    471/ 200000: 2.1353\n",
      "    472/ 200000: 2.3993\n",
      "    473/ 200000: 2.1912\n",
      "    474/ 200000: 2.6795\n",
      "    475/ 200000: 2.6175\n",
      "    476/ 200000: 3.0080\n",
      "    477/ 200000: 2.4737\n",
      "    478/ 200000: 2.4763\n",
      "    479/ 200000: 2.2090\n",
      "    480/ 200000: 2.7093\n",
      "    481/ 200000: 2.3527\n",
      "    482/ 200000: 2.3201\n",
      "    483/ 200000: 2.5368\n",
      "    484/ 200000: 2.2562\n",
      "    485/ 200000: 2.4297\n",
      "    486/ 200000: 2.5010\n",
      "    487/ 200000: 2.4938\n",
      "    488/ 200000: 2.2337\n",
      "    489/ 200000: 2.2674\n",
      "    490/ 200000: 2.1712\n",
      "    491/ 200000: 2.2320\n",
      "    492/ 200000: 2.4579\n",
      "    493/ 200000: 2.4695\n",
      "    494/ 200000: 2.1447\n",
      "    495/ 200000: 2.4941\n",
      "    496/ 200000: 3.0026\n",
      "    497/ 200000: 2.5699\n",
      "    498/ 200000: 2.3503\n",
      "    499/ 200000: 2.3821\n",
      "    500/ 200000: 2.3483\n",
      "    501/ 200000: 2.3392\n",
      "    502/ 200000: 2.5280\n",
      "    503/ 200000: 2.0848\n",
      "    504/ 200000: 2.3908\n",
      "    505/ 200000: 2.5164\n",
      "    506/ 200000: 2.4223\n",
      "    507/ 200000: 2.4614\n",
      "    508/ 200000: 2.3372\n",
      "    509/ 200000: 2.2773\n",
      "    510/ 200000: 2.5760\n",
      "    511/ 200000: 2.1221\n",
      "    512/ 200000: 2.3836\n",
      "    513/ 200000: 2.2668\n",
      "    514/ 200000: 2.3420\n",
      "    515/ 200000: 2.4448\n",
      "    516/ 200000: 2.5787\n",
      "    517/ 200000: 2.6477\n",
      "    518/ 200000: 2.4289\n",
      "    519/ 200000: 2.1969\n",
      "    520/ 200000: 2.4834\n",
      "    521/ 200000: 2.4310\n",
      "    522/ 200000: 2.1747\n",
      "    523/ 200000: 2.3909\n",
      "    524/ 200000: 2.8908\n",
      "    525/ 200000: 2.6509\n",
      "    526/ 200000: 2.6578\n",
      "    527/ 200000: 2.3523\n",
      "    528/ 200000: 2.5132\n",
      "    529/ 200000: 2.2186\n",
      "    530/ 200000: 2.6687\n",
      "    531/ 200000: 2.6908\n",
      "    532/ 200000: 2.3789\n",
      "    533/ 200000: 2.4593\n",
      "    534/ 200000: 2.4326\n",
      "    535/ 200000: 2.3690\n",
      "    536/ 200000: 2.3149\n",
      "    537/ 200000: 2.4023\n",
      "    538/ 200000: 2.5724\n",
      "    539/ 200000: 2.1718\n",
      "    540/ 200000: 2.6063\n",
      "    541/ 200000: 2.8026\n",
      "    542/ 200000: 2.5014\n",
      "    543/ 200000: 2.3418\n",
      "    544/ 200000: 2.4467\n",
      "    545/ 200000: 2.3669\n",
      "    546/ 200000: 2.5820\n",
      "    547/ 200000: 2.4063\n",
      "    548/ 200000: 2.3803\n",
      "    549/ 200000: 2.6265\n",
      "    550/ 200000: 2.4514\n",
      "    551/ 200000: 2.2605\n",
      "    552/ 200000: 2.2310\n",
      "    553/ 200000: 2.2664\n",
      "    554/ 200000: 2.6338\n",
      "    555/ 200000: 2.6961\n",
      "    556/ 200000: 2.1985\n",
      "    557/ 200000: 2.3402\n",
      "    558/ 200000: 2.0604\n",
      "    559/ 200000: 2.5131\n",
      "    560/ 200000: 2.4014\n",
      "    561/ 200000: 2.7413\n",
      "    562/ 200000: 2.4997\n",
      "    563/ 200000: 2.5571\n",
      "    564/ 200000: 2.2428\n",
      "    565/ 200000: 2.5090\n",
      "    566/ 200000: 2.3592\n",
      "    567/ 200000: 2.6209\n",
      "    568/ 200000: 2.0678\n",
      "    569/ 200000: 2.5974\n",
      "    570/ 200000: 2.2804\n",
      "    571/ 200000: 2.4360\n",
      "    572/ 200000: 2.4533\n",
      "    573/ 200000: 2.2954\n",
      "    574/ 200000: 2.2719\n",
      "    575/ 200000: 2.3070\n",
      "    576/ 200000: 2.6097\n",
      "    577/ 200000: 2.5642\n",
      "    578/ 200000: 2.6463\n",
      "    579/ 200000: 2.2376\n",
      "    580/ 200000: 2.2625\n",
      "    581/ 200000: 2.4351\n",
      "    582/ 200000: 2.5350\n",
      "    583/ 200000: 2.2944\n",
      "    584/ 200000: 2.8977\n",
      "    585/ 200000: 2.4243\n",
      "    586/ 200000: 2.5346\n",
      "    587/ 200000: 2.6900\n",
      "    588/ 200000: 2.2831\n",
      "    589/ 200000: 2.4447\n",
      "    590/ 200000: 2.3396\n",
      "    591/ 200000: 2.2824\n",
      "    592/ 200000: 2.8340\n",
      "    593/ 200000: 2.5071\n",
      "    594/ 200000: 2.3721\n",
      "    595/ 200000: 2.1231\n",
      "    596/ 200000: 2.3011\n",
      "    597/ 200000: 2.4570\n",
      "    598/ 200000: 2.2868\n",
      "    599/ 200000: 2.5335\n",
      "    600/ 200000: 2.3441\n",
      "    601/ 200000: 2.4458\n",
      "    602/ 200000: 2.3564\n",
      "    603/ 200000: 2.3813\n",
      "    604/ 200000: 2.5492\n",
      "    605/ 200000: 2.1433\n",
      "    606/ 200000: 2.0391\n",
      "    607/ 200000: 2.5721\n",
      "    608/ 200000: 2.3889\n",
      "    609/ 200000: 2.4928\n",
      "    610/ 200000: 2.7029\n",
      "    611/ 200000: 2.5173\n",
      "    612/ 200000: 2.1676\n",
      "    613/ 200000: 2.5569\n",
      "    614/ 200000: 2.3919\n",
      "    615/ 200000: 2.4772\n",
      "    616/ 200000: 2.4161\n",
      "    617/ 200000: 2.2052\n",
      "    618/ 200000: 2.6451\n",
      "    619/ 200000: 2.5348\n",
      "    620/ 200000: 2.3988\n",
      "    621/ 200000: 2.7985\n",
      "    622/ 200000: 2.4011\n",
      "    623/ 200000: 2.2415\n",
      "    624/ 200000: 2.4704\n",
      "    625/ 200000: 2.7686\n",
      "    626/ 200000: 2.4097\n",
      "    627/ 200000: 2.2391\n",
      "    628/ 200000: 2.4099\n",
      "    629/ 200000: 2.3244\n",
      "    630/ 200000: 2.5460\n",
      "    631/ 200000: 2.6272\n",
      "    632/ 200000: 2.0884\n",
      "    633/ 200000: 2.5216\n",
      "    634/ 200000: 2.4070\n",
      "    635/ 200000: 2.2051\n",
      "    636/ 200000: 2.9606\n",
      "    637/ 200000: 2.4934\n",
      "    638/ 200000: 2.6092\n",
      "    639/ 200000: 2.5852\n",
      "    640/ 200000: 2.3108\n",
      "    641/ 200000: 2.0931\n",
      "    642/ 200000: 2.3538\n",
      "    643/ 200000: 2.2063\n",
      "    644/ 200000: 2.2337\n",
      "    645/ 200000: 2.1113\n",
      "    646/ 200000: 2.1639\n",
      "    647/ 200000: 2.7637\n",
      "    648/ 200000: 2.1599\n",
      "    649/ 200000: 2.4882\n",
      "    650/ 200000: 2.4201\n",
      "    651/ 200000: 2.1558\n",
      "    652/ 200000: 2.4872\n",
      "    653/ 200000: 2.2128\n",
      "    654/ 200000: 2.3962\n",
      "    655/ 200000: 2.3455\n",
      "    656/ 200000: 2.5791\n",
      "    657/ 200000: 2.3475\n",
      "    658/ 200000: 2.4043\n",
      "    659/ 200000: 2.4341\n",
      "    660/ 200000: 2.4143\n",
      "    661/ 200000: 2.4555\n",
      "    662/ 200000: 2.4017\n",
      "    663/ 200000: 2.1612\n",
      "    664/ 200000: 2.5195\n",
      "    665/ 200000: 2.7484\n",
      "    666/ 200000: 2.2717\n",
      "    667/ 200000: 2.3920\n",
      "    668/ 200000: 2.8904\n",
      "    669/ 200000: 2.6159\n",
      "    670/ 200000: 2.3112\n",
      "    671/ 200000: 2.5761\n",
      "    672/ 200000: 2.2895\n",
      "    673/ 200000: 2.6796\n",
      "    674/ 200000: 2.1055\n",
      "    675/ 200000: 2.0726\n",
      "    676/ 200000: 2.4169\n",
      "    677/ 200000: 2.4778\n",
      "    678/ 200000: 2.3998\n",
      "    679/ 200000: 2.5641\n",
      "    680/ 200000: 2.3982\n",
      "    681/ 200000: 2.5140\n",
      "    682/ 200000: 2.2827\n",
      "    683/ 200000: 2.2302\n",
      "    684/ 200000: 2.3395\n",
      "    685/ 200000: 2.2473\n",
      "    686/ 200000: 2.7095\n",
      "    687/ 200000: 2.5030\n",
      "    688/ 200000: 2.7020\n",
      "    689/ 200000: 1.9868\n",
      "    690/ 200000: 2.6014\n",
      "    691/ 200000: 2.4045\n",
      "    692/ 200000: 2.5845\n",
      "    693/ 200000: 2.0226\n",
      "    694/ 200000: 2.2960\n",
      "    695/ 200000: 2.6179\n",
      "    696/ 200000: 2.2111\n",
      "    697/ 200000: 2.4049\n",
      "    698/ 200000: 2.3138\n",
      "    699/ 200000: 1.9926\n",
      "    700/ 200000: 2.3902\n",
      "    701/ 200000: 2.3042\n",
      "    702/ 200000: 2.5040\n",
      "    703/ 200000: 2.0527\n",
      "    704/ 200000: 2.3233\n",
      "    705/ 200000: 2.4874\n",
      "    706/ 200000: 2.8381\n",
      "    707/ 200000: 2.2458\n",
      "    708/ 200000: 2.2542\n",
      "    709/ 200000: 2.7014\n",
      "    710/ 200000: 2.9553\n",
      "    711/ 200000: 2.4999\n",
      "    712/ 200000: 2.3243\n",
      "    713/ 200000: 2.5800\n",
      "    714/ 200000: 2.1904\n",
      "    715/ 200000: 2.2954\n",
      "    716/ 200000: 2.1904\n",
      "    717/ 200000: 2.4219\n",
      "    718/ 200000: 2.4313\n",
      "    719/ 200000: 2.3240\n",
      "    720/ 200000: 1.9614\n",
      "    721/ 200000: 2.4322\n",
      "    722/ 200000: 2.5503\n",
      "    723/ 200000: 2.2664\n",
      "    724/ 200000: 2.5127\n",
      "    725/ 200000: 2.7271\n",
      "    726/ 200000: 2.4117\n",
      "    727/ 200000: 1.9764\n",
      "    728/ 200000: 2.6679\n",
      "    729/ 200000: 2.2684\n",
      "    730/ 200000: 2.0577\n",
      "    731/ 200000: 2.5832\n",
      "    732/ 200000: 2.4974\n",
      "    733/ 200000: 2.1675\n",
      "    734/ 200000: 2.2375\n",
      "    735/ 200000: 2.3865\n",
      "    736/ 200000: 2.5767\n",
      "    737/ 200000: 2.3695\n",
      "    738/ 200000: 2.3864\n",
      "    739/ 200000: 2.3139\n",
      "    740/ 200000: 2.5348\n",
      "    741/ 200000: 2.3266\n",
      "    742/ 200000: 2.6200\n",
      "    743/ 200000: 2.4377\n",
      "    744/ 200000: 2.4035\n",
      "    745/ 200000: 2.4632\n",
      "    746/ 200000: 2.5411\n",
      "    747/ 200000: 2.4319\n",
      "    748/ 200000: 2.2930\n",
      "    749/ 200000: 2.5151\n",
      "    750/ 200000: 2.3691\n",
      "    751/ 200000: 2.1732\n",
      "    752/ 200000: 2.3852\n",
      "    753/ 200000: 2.7448\n",
      "    754/ 200000: 2.8644\n",
      "    755/ 200000: 2.2790\n",
      "    756/ 200000: 2.4036\n",
      "    757/ 200000: 2.2924\n",
      "    758/ 200000: 2.1906\n",
      "    759/ 200000: 2.7164\n",
      "    760/ 200000: 2.3415\n",
      "    761/ 200000: 2.5149\n",
      "    762/ 200000: 2.4666\n",
      "    763/ 200000: 2.4081\n",
      "    764/ 200000: 2.5501\n",
      "    765/ 200000: 2.0473\n",
      "    766/ 200000: 2.0937\n",
      "    767/ 200000: 2.5362\n",
      "    768/ 200000: 2.4937\n",
      "    769/ 200000: 2.2907\n",
      "    770/ 200000: 2.2917\n",
      "    771/ 200000: 2.3831\n",
      "    772/ 200000: 2.5936\n",
      "    773/ 200000: 2.4691\n",
      "    774/ 200000: 2.2483\n",
      "    775/ 200000: 2.4946\n",
      "    776/ 200000: 2.3472\n",
      "    777/ 200000: 2.2926\n",
      "    778/ 200000: 2.5196\n",
      "    779/ 200000: 2.3108\n",
      "    780/ 200000: 2.4264\n",
      "    781/ 200000: 2.4654\n",
      "    782/ 200000: 2.2779\n",
      "    783/ 200000: 2.4617\n",
      "    784/ 200000: 2.1016\n",
      "    785/ 200000: 2.3926\n",
      "    786/ 200000: 2.5819\n",
      "    787/ 200000: 2.3445\n",
      "    788/ 200000: 2.4115\n",
      "    789/ 200000: 2.3771\n",
      "    790/ 200000: 2.1247\n",
      "    791/ 200000: 2.2418\n",
      "    792/ 200000: 2.2259\n",
      "    793/ 200000: 2.2268\n",
      "    794/ 200000: 2.3908\n",
      "    795/ 200000: 2.7926\n",
      "    796/ 200000: 2.3426\n",
      "    797/ 200000: 2.1239\n",
      "    798/ 200000: 2.7109\n",
      "    799/ 200000: 2.4755\n",
      "    800/ 200000: 2.1450\n",
      "    801/ 200000: 2.5912\n",
      "    802/ 200000: 2.6183\n",
      "    803/ 200000: 2.4295\n",
      "    804/ 200000: 2.2816\n",
      "    805/ 200000: 2.1556\n",
      "    806/ 200000: 2.9322\n",
      "    807/ 200000: 2.4059\n",
      "    808/ 200000: 2.3206\n",
      "    809/ 200000: 1.9766\n",
      "    810/ 200000: 2.4586\n",
      "    811/ 200000: 2.5022\n",
      "    812/ 200000: 2.7958\n",
      "    813/ 200000: 2.4842\n",
      "    814/ 200000: 2.2869\n",
      "    815/ 200000: 2.1784\n",
      "    816/ 200000: 2.1276\n",
      "    817/ 200000: 2.2579\n",
      "    818/ 200000: 2.6276\n",
      "    819/ 200000: 2.2559\n",
      "    820/ 200000: 2.2689\n",
      "    821/ 200000: 2.0266\n",
      "    822/ 200000: 2.4934\n",
      "    823/ 200000: 2.4362\n",
      "    824/ 200000: 2.3663\n",
      "    825/ 200000: 2.0189\n",
      "    826/ 200000: 2.0776\n",
      "    827/ 200000: 2.3448\n",
      "    828/ 200000: 2.0020\n",
      "    829/ 200000: 2.5938\n",
      "    830/ 200000: 2.4466\n",
      "    831/ 200000: 2.2011\n",
      "    832/ 200000: 2.5758\n",
      "    833/ 200000: 2.3142\n",
      "    834/ 200000: 2.5210\n",
      "    835/ 200000: 2.3143\n",
      "    836/ 200000: 2.4828\n",
      "    837/ 200000: 2.2961\n",
      "    838/ 200000: 2.6033\n",
      "    839/ 200000: 2.4813\n",
      "    840/ 200000: 2.2817\n",
      "    841/ 200000: 2.2187\n",
      "    842/ 200000: 2.1386\n",
      "    843/ 200000: 2.3356\n",
      "    844/ 200000: 2.3529\n",
      "    845/ 200000: 2.3528\n",
      "    846/ 200000: 2.5489\n",
      "    847/ 200000: 2.2667\n",
      "    848/ 200000: 2.4255\n",
      "    849/ 200000: 2.2495\n",
      "    850/ 200000: 2.2854\n",
      "    851/ 200000: 2.3539\n",
      "    852/ 200000: 2.4667\n",
      "    853/ 200000: 2.4032\n",
      "    854/ 200000: 2.3351\n",
      "    855/ 200000: 2.2460\n",
      "    856/ 200000: 2.2471\n",
      "    857/ 200000: 2.7737\n",
      "    858/ 200000: 2.3926\n",
      "    859/ 200000: 2.7454\n",
      "    860/ 200000: 2.2615\n",
      "    861/ 200000: 2.2299\n",
      "    862/ 200000: 2.0990\n",
      "    863/ 200000: 2.3539\n",
      "    864/ 200000: 2.3521\n",
      "    865/ 200000: 2.4112\n",
      "    866/ 200000: 2.2054\n",
      "    867/ 200000: 2.6552\n",
      "    868/ 200000: 2.2803\n",
      "    869/ 200000: 2.7162\n",
      "    870/ 200000: 2.0881\n",
      "    871/ 200000: 2.3091\n",
      "    872/ 200000: 2.8953\n",
      "    873/ 200000: 2.1901\n",
      "    874/ 200000: 2.5098\n",
      "    875/ 200000: 2.4640\n",
      "    876/ 200000: 2.5937\n",
      "    877/ 200000: 2.2287\n",
      "    878/ 200000: 2.4032\n",
      "    879/ 200000: 2.4301\n",
      "    880/ 200000: 2.6962\n",
      "    881/ 200000: 2.3063\n",
      "    882/ 200000: 2.2066\n",
      "    883/ 200000: 2.7117\n",
      "    884/ 200000: 2.6379\n",
      "    885/ 200000: 2.3619\n",
      "    886/ 200000: 2.7774\n",
      "    887/ 200000: 2.3227\n",
      "    888/ 200000: 2.6898\n",
      "    889/ 200000: 2.3835\n",
      "    890/ 200000: 2.4493\n",
      "    891/ 200000: 2.4995\n",
      "    892/ 200000: 2.2072\n",
      "    893/ 200000: 2.3508\n",
      "    894/ 200000: 2.2352\n",
      "    895/ 200000: 2.0320\n",
      "    896/ 200000: 2.2428\n",
      "    897/ 200000: 2.5228\n",
      "    898/ 200000: 2.6449\n",
      "    899/ 200000: 2.4222\n",
      "    900/ 200000: 2.3190\n",
      "    901/ 200000: 2.6720\n",
      "    902/ 200000: 2.1439\n",
      "    903/ 200000: 2.6710\n",
      "    904/ 200000: 1.9864\n",
      "    905/ 200000: 2.3988\n",
      "    906/ 200000: 2.1583\n",
      "    907/ 200000: 2.6165\n",
      "    908/ 200000: 2.3071\n",
      "    909/ 200000: 2.3834\n",
      "    910/ 200000: 2.5169\n",
      "    911/ 200000: 2.5849\n",
      "    912/ 200000: 2.4477\n",
      "    913/ 200000: 2.4277\n",
      "    914/ 200000: 2.2431\n",
      "    915/ 200000: 1.9945\n",
      "    916/ 200000: 2.0273\n",
      "    917/ 200000: 1.8721\n",
      "    918/ 200000: 2.3560\n",
      "    919/ 200000: 2.6211\n",
      "    920/ 200000: 2.4315\n",
      "    921/ 200000: 2.4490\n",
      "    922/ 200000: 2.3517\n",
      "    923/ 200000: 2.4386\n",
      "    924/ 200000: 2.3077\n",
      "    925/ 200000: 2.4541\n",
      "    926/ 200000: 2.4728\n",
      "    927/ 200000: 2.4668\n",
      "    928/ 200000: 2.0723\n",
      "    929/ 200000: 2.2280\n",
      "    930/ 200000: 2.6640\n",
      "    931/ 200000: 2.4672\n",
      "    932/ 200000: 2.1190\n",
      "    933/ 200000: 2.5237\n",
      "    934/ 200000: 2.3705\n",
      "    935/ 200000: 2.1492\n",
      "    936/ 200000: 2.8295\n",
      "    937/ 200000: 2.1330\n",
      "    938/ 200000: 2.4764\n",
      "    939/ 200000: 2.3604\n",
      "    940/ 200000: 2.4020\n",
      "    941/ 200000: 2.4502\n",
      "    942/ 200000: 2.2779\n",
      "    943/ 200000: 2.2629\n",
      "    944/ 200000: 2.3277\n",
      "    945/ 200000: 2.6493\n",
      "    946/ 200000: 2.3560\n",
      "    947/ 200000: 2.5167\n",
      "    948/ 200000: 2.7861\n",
      "    949/ 200000: 2.1186\n",
      "    950/ 200000: 2.5257\n",
      "    951/ 200000: 2.4873\n",
      "    952/ 200000: 2.4608\n",
      "    953/ 200000: 2.4718\n",
      "    954/ 200000: 2.6317\n",
      "    955/ 200000: 2.2991\n",
      "    956/ 200000: 2.3951\n",
      "    957/ 200000: 2.3447\n",
      "    958/ 200000: 2.2753\n",
      "    959/ 200000: 1.9041\n",
      "    960/ 200000: 2.7471\n",
      "    961/ 200000: 2.6304\n",
      "    962/ 200000: 2.7048\n",
      "    963/ 200000: 2.3700\n",
      "    964/ 200000: 2.3552\n",
      "    965/ 200000: 2.7688\n",
      "    966/ 200000: 2.0764\n",
      "    967/ 200000: 2.2590\n",
      "    968/ 200000: 2.1301\n",
      "    969/ 200000: 2.2930\n",
      "    970/ 200000: 2.2979\n",
      "    971/ 200000: 2.1083\n",
      "    972/ 200000: 2.6838\n",
      "    973/ 200000: 2.6731\n",
      "    974/ 200000: 2.3579\n",
      "    975/ 200000: 2.3429\n",
      "    976/ 200000: 2.3427\n",
      "    977/ 200000: 2.4863\n",
      "    978/ 200000: 2.9379\n",
      "    979/ 200000: 2.4338\n",
      "    980/ 200000: 2.4709\n",
      "    981/ 200000: 2.5358\n",
      "    982/ 200000: 2.3911\n",
      "    983/ 200000: 2.2034\n",
      "    984/ 200000: 2.3275\n",
      "    985/ 200000: 2.3711\n",
      "    986/ 200000: 2.5037\n",
      "    987/ 200000: 2.2354\n",
      "    988/ 200000: 2.5892\n",
      "    989/ 200000: 1.9803\n",
      "    990/ 200000: 2.5961\n",
      "    991/ 200000: 2.4174\n",
      "    992/ 200000: 2.6077\n",
      "    993/ 200000: 2.3927\n",
      "    994/ 200000: 2.0438\n",
      "    995/ 200000: 2.2222\n",
      "    996/ 200000: 2.5131\n",
      "    997/ 200000: 2.2176\n",
      "    998/ 200000: 2.7675\n",
      "    999/ 200000: 2.4339\n",
      "   1000/ 200000: 2.8839\n"
     ]
    }
   ],
   "source": [
    "max_steps = 200000\n",
    "batch_size = 32\n",
    "lossi, ud = [], []\n",
    "\n",
    "for i in range(max_steps):\n",
    "    #minibatch construct\n",
    "    ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "    Xb, Yb = Xtr[ix], Ytr[ix]\n",
    "\n",
    "    emb = C[Xb]\n",
    "    x = emb.view(Xb.shape[0], -1)\n",
    "    for layer in layers:\n",
    "        x = layer(x)\n",
    "\n",
    "    loss = F.cross_entropy(x, Yb)\n",
    "\n",
    "    #backward pass\n",
    "    for layer in layers:\n",
    "        layer.out.retain_grad()\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    #update\n",
    "    lr = 0.1 if i < 100_000 else 0.01\n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad\n",
    "\n",
    "    #track stats\n",
    "    if i % 10000:\n",
    "        print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        #capturing the ration of update to data, ideal is ~ 1e-03\n",
    "        ud.append([((lr*p.grad.std() / p.data.std()).log10().item() for p in parameters)])\n",
    "\n",
    "    if i >= 1000:\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b69a03e-1790-4a43-99eb-91d8b74e8c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize histograms\n",
    "plt.figure(figsize=(20, 4)) # width and height of the plot\n",
    "legends = []\n",
    "for i, layer in enumerate(layers[:-1]): # note: exclude the output layer\n",
    "  if isinstance(layer, Tanh):\n",
    "    t = layer.out\n",
    "    print('layer %d (%10s): mean %+.2f, std %.2f, saturated: %.2f%%' % (i, layer.__class__.__name__, t.mean(), t.std(), (t.abs() > 0.97).float().mean()*100))\n",
    "    hy, hx = torch.histogram(t, density=True)\n",
    "    plt.plot(hx[:-1].detach(), hy.detach())\n",
    "    legends.append(f'layer {i} ({layer.__class__.__name__}')\n",
    "plt.legend(legends);\n",
    "plt.title('activation distribution')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
